import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import requests

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score


from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    precision_recall_fscore_support,
    accuracy_score
)

import statsmodels.api as sm
from scipy.stats import zscore






import pandas as pd

# Load data from the Excel file
file_path = 'data.xlsx'  # Adjust the file path as needed
df = pd.read_excel(file_path)

# Specify the feature and target column names based on the dataset
feature_columns = ['AT', 'V', 'AP', 'RH']  # Feature columns
target_column = 'PE'  # Target column for energy output

# Separate features and target columns
df_features = df[feature_columns]
df_target = df[[target_column]]

# Concatenate features and target into a single DataFrame
df = pd.concat([df_features, df_target], axis=1)

# Display the final DataFrame
print("Combined DataFrame with features and target:")
print(df)






df


# Generate pairwise scatterplots including predictors and the target variable
sns.pairplot(df)
plt.suptitle("Pairwise Scatterplots of Features and Target Variable", y=1.02)
plt.show()






# Calculate summary statistics
summary_stats = pd.DataFrame({
    'mean': df.mean(),
    'median': df.median(),
    'range': df.max() - df.min(),
    'Q1': df.quantile(0.25),
    'Q3': df.quantile(0.75),
    'IQR': df.quantile(0.75) - df.quantile(0.25)
})

print(summary_stats)



df.columns





y = df['PE']  # Response variable
predictors = ['AT', 'V', 'AP', 'RH']  # Predictor variables


# Function to perform simple linear regression and return coefficient statistics
def perform_regression_and_test(X, y):
    X = sm.add_constant(X)  # Add intercept
    model = sm.OLS(y, X).fit()
    return {
        'Coefficient': model.params[1],
        'T-value': model.tvalues[1],
        'P-value': model.pvalues[1],
        'R² Score': model.rsquared
    }

# Dictionary to store results for univariate regression
results = {}

# Loop through each predictor, fit the model, and store results
for predictor in predictors:
    X = df[[predictor]]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    result = perform_regression_and_test(X_train, y_train)
    results[predictor] = result



# Display results for each univariate model
for predictor, metrics in results.items():
    print(f"Results for predictor: {predictor}")
    for key, value in metrics.items():
        print(f"  {key}: {value}")
    print("\n")






# Function to calculate residuals and identify outliers based on z-scores
def calculate_residuals_and_outliers(X, y):
    X = sm.add_constant(X)
    model = sm.OLS(y, X).fit()
    predictions = model.predict(X)
    residuals = y - predictions
    residuals_z_scores = zscore(residuals)
    outliers = np.abs(residuals_z_scores) > 3
    return residuals, outliers

# Plot residuals with highlighted outliers for each predictor
for predictor in predictors:
    X = df[[predictor]]
    residuals, outliers = calculate_residuals_and_outliers(X, y)
    plt.figure(figsize=(10, 6))
    plt.scatter(X, residuals, label="Residuals", color="blue")
    plt.hlines(0, X.min(), X.max(), colors="red", linestyles="--", label="Zero Line")
    plt.scatter(X[outliers], residuals[outliers], color="orange", label="Outliers", marker="x")
    plt.xlabel(predictor)
    plt.ylabel("Residuals")
    plt.title(f"Residuals Plot for {predictor} vs PE")
    plt.legend()
    plt.show()






# Prepare predictor variables and response variable
X = df[['AT', 'V', 'AP', 'RH']]
y = df['PE']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add constant to training data for the intercept
X_train = sm.add_constant(X_train)

# Fit the multiple regression model
model_multivariate = sm.OLS(y_train, X_train).fit()

# Display the summary for the multiple regression model
print("Multiple Regression Model Summary:")
print(model_multivariate.summary())






# Add constant to the test set and predict
X_test = sm.add_constant(X_test)
y_pred = model_multivariate.predict(X_test)

# Calculate the test set R² score
test_r2 = sm.OLS(y_test, y_pred).fit().rsquared
print(f"Test R² Score: {test_r2}")






# Collect coefficients from the multiple regression model
multivariate_coefficients = model_multivariate.params[1:]  # Skip intercept

# Prepare data for comparison plot
univariate_coefficients = [results[predictor]['Coefficient'] for predictor in predictors]
multivariate_coefficients = multivariate_coefficients.values

# Plot comparison of univariate and multivariate coefficients
plt.figure(figsize=(8, 6))
plt.scatter(univariate_coefficients, multivariate_coefficients, color='blue')

# Annotate each point with the predictor name
for i, predictor in enumerate(predictors):
    plt.text(univariate_coefficients[i], multivariate_coefficients[i], predictor, fontsize=12)

# Add labels and title
plt.xlabel("Univariate Regression Coefficients")
plt.ylabel("Multiple Regression Coefficients")
plt.title("Comparison of Univariate and Multiple Regression Coefficients")
plt.axhline(0, color='gray', linestyle='--')
plt.axvline(0, color='gray', linestyle='--')
plt.show()









from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

# Response variable
y = df['PE']

# Dictionary to store results
polynomial_results = {}

# Loop through each predictor to fit a polynomial regression model
for predictor in predictors:
    # Generate polynomial features up to degree 3 (X, X^2, X^3)
    X = df[[predictor]]
    poly = PolynomialFeatures(degree=3, include_bias=False)
    X_poly = poly.fit_transform(X)
    
    # Fit the model using statsmodels
    X_poly = sm.add_constant(X_poly)  # Add intercept
    model = sm.OLS(y, X_poly).fit()
    
    # Store summary results for analysis
    polynomial_results[predictor] = model.summary()
    print(f"Polynomial Regression Model Summary for {predictor} (up to degree 3):")
    print(model.summary())
    print("\n")









from itertools import combinations

# Generate pairwise interaction terms for predictors
interaction_terms = []
for (p1, p2) in combinations(predictors, 2):
    interaction_name = f"{p1}*{p2}"
    df[interaction_name] = df[p1] * df[p2]
    interaction_terms.append(interaction_name)

# Define predictors with interaction terms included
X_interactions = df[predictors + interaction_terms]
X_interactions = sm.add_constant(X_interactions)

# Fit the model with interaction terms
interaction_model = sm.OLS(y, X_interactions).fit()

# Display the summary to evaluate significance of interaction terms
print("Full Model with Interaction Terms Summary:")
print(interaction_model.summary())









from sklearn.metrics import mean_squared_error

# Define predictors and response variable
X = df[['AT', 'V', 'AP', 'RH']]
y = df['PE']

# Split the data: 70% for training, 30% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)



from sklearn.preprocessing import PolynomialFeatures
import statsmodels.api as sm

# Generate polynomial features up to degree 2 (includes quadratic terms and interactions)
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Fit the model on training data
X_train_poly = sm.add_constant(X_train_poly)  # Add intercept
model1 = sm.OLS(y_train, X_train_poly).fit()

# Predict and calculate train and test MSE for Model 1
y_train_pred_model1 = model1.predict(X_train_poly)
train_mse_model1 = mean_squared_error(y_train, y_train_pred_model1)

X_test_poly = sm.add_constant(X_test_poly)  # Add intercept to test data
y_test_pred_model1 = model1.predict(X_test_poly)
test_mse_model1 = mean_squared_error(y_test, y_test_pred_model1)

print(f"Model 1 - Train MSE: {train_mse_model1}, Test MSE: {test_mse_model1}")



# Re-initialize X_train_poly and X_test_poly with all quadratic and interaction terms
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Convert to DataFrame to track columns
X_train_poly_df = pd.DataFrame(X_train_poly, columns=poly.get_feature_names_out(X.columns))
X_train_poly_df = sm.add_constant(X_train_poly_df)  # Add intercept
y_train = y_train.reset_index(drop=True)  # Reset index for y_train

# Fit the initial full model with all terms
model2 = sm.OLS(y_train, X_train_poly_df).fit()

# Iteratively remove predictors with p-value > 0.05
while True:
    p_values = model2.pvalues
    max_p = p_values.max()  # Find max p-value
    if max_p > 0.05:
        feature_to_remove = p_values.idxmax()  # Identify the feature to remove
        X_train_poly_df = X_train_poly_df.drop(columns=[feature_to_remove])
        model2 = sm.OLS(y_train, X_train_poly_df).fit()  # Refit model
    else:
        break  # Stop if all p-values are significant

# Prepare test set with the same features as the final training set
X_test_poly_df = pd.DataFrame(X_test_poly, columns=poly.get_feature_names_out(X.columns))
X_test_poly_df = sm.add_constant(X_test_poly_df)
X_test_poly_df = X_test_poly_df[X_train_poly_df.columns]
y_test = y_test.reset_index(drop=True)  # Reset index for y_test

# Predict and calculate train and test MSE for Model 2
y_train_pred_model2 = model2.predict(X_train_poly_df)
train_mse_model2 = mean_squared_error(y_train, y_train_pred_model2)

y_test_pred_model2 = model2.predict(X_test_poly_df)
test_mse_model2 = mean_squared_error(y_test, y_test_pred_model2)

print(f"Model 2 - Train MSE: {train_mse_model2}, Test MSE: {test_mse_model2}")












from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt

# Define the predictors and response variable
X = df[['AT', 'V', 'AP', 'RH']]
y = df['PE']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Normalize the features
scaler = StandardScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)



# Lists to store train and test MSEs for both raw and normalized features
train_mse_raw = []
test_mse_raw = []
train_mse_norm = []
test_mse_norm = []

# Range of k values to try
k_values = range(1, 101)

for k in k_values:
    # KNN with raw features
    knn_raw = KNeighborsRegressor(n_neighbors=k)
    knn_raw.fit(X_train, y_train)
    y_train_pred_raw = knn_raw.predict(X_train)
    y_test_pred_raw = knn_raw.predict(X_test)
    train_mse_raw.append(mean_squared_error(y_train, y_train_pred_raw))
    test_mse_raw.append(mean_squared_error(y_test, y_test_pred_raw))
    
    # KNN with normalized features
    knn_norm = KNeighborsRegressor(n_neighbors=k)
    knn_norm.fit(X_train_norm, y_train)
    y_train_pred_norm = knn_norm.predict(X_train_norm)
    y_test_pred_norm = knn_norm.predict(X_test_norm)
    train_mse_norm.append(mean_squared_error(y_train, y_train_pred_norm))
    test_mse_norm.append(mean_squared_error(y_test, y_test_pred_norm))



plt.figure(figsize=(14, 6))

# Plot for raw features
plt.subplot(1, 2, 1)
plt.plot(1 / np.array(k_values), train_mse_raw, label="Train MSE (Raw Features)", color='blue')
plt.plot(1 / np.array(k_values), test_mse_raw, label="Test MSE (Raw Features)", color='red')
plt.xlabel("1 / k")
plt.ylabel("Mean Squared Error")
plt.title("KNN Regression (Raw Features)")
plt.legend()

# Plot for normalized features
plt.subplot(1, 2, 2)
plt.plot(1 / np.array(k_values), train_mse_norm, label="Train MSE (Normalized Features)", color='blue')
plt.plot(1 / np.array(k_values), test_mse_norm, label="Test MSE (Normalized Features)", color='red')
plt.xlabel("1 / k")
plt.ylabel("Mean Squared Error")
plt.title("KNN Regression (Normalized Features)")
plt.legend()

plt.tight_layout()
plt.show()



# Optimal k for raw features
optimal_k_raw = k_values[np.argmin(test_mse_raw)]
print(f"Optimal k (Raw Features): {optimal_k_raw}, Test MSE: {min(test_mse_raw)}")

# Optimal k for normalized features
optimal_k_norm = k_values[np.argmin(test_mse_norm)]
print(f"Optimal k (Normalized Features): {optimal_k_norm}, Test MSE: {min(test_mse_norm)}")







